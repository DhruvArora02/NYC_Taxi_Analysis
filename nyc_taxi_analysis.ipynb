{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Trip Data Analysis with Databricks (PySpark & SQL)\n",
    "\n",
    "This notebook demonstrates basic data analysis using PySpark in Databricks.\n",
    "We use a small NYC taxi dataset to:\n",
    "- Load data from CSV\n",
    "- Explore data structure\n",
    "- Perform basic transformations\n",
    "- Run SQL queries\n",
    "- Create simple visualizations\n",
    "\n",
    "**Author:** Your Name\n",
    "**Date:** 2025-08-13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file into Databricks\n",
    "df = spark.read.csv(\"/FileStore/nyc_taxi_sample.csv\", header=True, inferSchema=True)\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "We'll check for nulls and basic descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, isnan\n",
    "\n",
    "# Count missing values per column\n",
    "df.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# Show basic stats\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "We add a `trip_duration_minutes` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, round\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"trip_duration_minutes\",\n",
    "    round((unix_timestamp(col(\"dropoff_datetime\")) - unix_timestamp(col(\"pickup_datetime\"))) / 60, 2)\n",
    ")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register as SQL Table and Query\n",
    "We can use Spark SQL to run queries on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"nyc_taxi\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT pickup_location, COUNT(*) as trip_count, ROUND(AVG(fare_amount), 2) as avg_fare\n",
    "FROM nyc_taxi\n",
    "GROUP BY pickup_location\n",
    "ORDER BY trip_count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Example\n",
    "We'll visualize average fare per pickup location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pdf = df.groupBy(\"pickup_location\").avg(\"fare_amount\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(pdf[\"pickup_location\"], pdf[\"avg(fare_amount)\"])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Pickup Location\")\n",
    "plt.ylabel(\"Average Fare ($)\")\n",
    "plt.title(\"Average Fare per Pickup Location\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
